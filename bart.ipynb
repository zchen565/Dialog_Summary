{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_metric,Dataset,DatasetDict\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"6,7\"\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "metric = load_metric(\"rouge.py\")\n",
    "\n",
    "TEST_SUMMARY_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_single_dialogsumm_file(file):\n",
    "#     data = open(file,\"r\").readlines()\n",
    "#     result = {\"fname\":[],\"summary\":[],\"dialogue\":[]}\n",
    "#     for i in data:\n",
    "#         d = json.loads(i)\n",
    "#         for j in d.keys():\n",
    "#             if j in result.keys():\n",
    "#                 result[j].append(d[j])\n",
    "#     return Dataset.from_dict(result)\n",
    "\n",
    "# def transform_test_file(file):\n",
    "#     data = open(file,\"r\").readlines()\n",
    "#     result = {\"fname\":[],\"summary%d\"%TEST_SUMMARY_ID:[],\"dialogue\":[]}\n",
    "#     for i in data:\n",
    "#         d = json.loads(i)\n",
    "#         for j in d.keys():\n",
    "#             if j in result.keys():\n",
    "#                 result[j].append(d[j])\n",
    "    \n",
    "#     result[\"summary\"] = result[\"summary%d\"%TEST_SUMMARY_ID]\n",
    "#     return Dataset.from_dict(result)\n",
    "\n",
    "# def transform_dialogsumm_to_huggingface_dataset(train,validation,test):\n",
    "#     train = transform_single_dialogsumm_file(train)\n",
    "#     validation = transform_single_dialogsumm_file(validation)\n",
    "#     test = transform_test_file(test)\n",
    "#     return DatasetDict({\"train\":train,\"validation\":validation,\"test\":test})\n",
    "\n",
    "# raw_datasets = transform_dialogsumm_to_huggingface_dataset(\"../DialogSum_Data/dialogsum.train.jsonl\",\"../DialogSum_Data/dialogsum.dev.jsonl\",\"../DialogSum_Data/dialogsum.test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"knkarthick/dialogsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-LARGE-DIALOGSUM\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_rouge1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    "    generation_max_length=max_target_length,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "out = trainer.predict(tokenized_datasets[\"test\"],num_beams=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels ,metric= out\n",
    "print(metric)\n",
    "\n",
    "\n",
    "decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge \n",
    "decoded_preds = [\" \".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "decoded_labels = [\" \".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bart_output.txt\",\"w\") as f: \n",
    "    for i in decoded_preds:\n",
    "        print(i)\n",
    "        f.write(i.replace(\"\\n\",\"\")+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
